[package]
name = "inference-lab"
version = "0.4.2"
edition = "2021"
authors = ["Doubleword"]
description = "High-performance LLM inference simulator for analyzing serving systems"
license = "MIT"
repository = "https://github.com/doublewordai/inference-lab"
readme = "README.md"
keywords = ["llm", "inference", "simulator", "performance"]
categories = ["simulation", "wasm"]
exclude = ["pkg/", ".npmrc", "node_modules/"]

[lib]
crate-type = ["cdylib", "rlib"]

[dependencies]
toml = "0.8"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
clap = { version = "4.5", features = ["derive"], optional = true }
rand = "0.8"
rand_distr = "0.4"
ordered-float = "4.0"
env_logger = { version = "0.11", optional = true }
log = "0.4"
tabled = { version = "0.16", optional = true }
colored = { version = "2.1", optional = true }
tokenizers = { version = "0.22", optional = true }
minijinja = { version = "2.0", optional = true }
tokio = { version = "1", features = ["full"], optional = true }
axum = { version = "0.8", optional = true }
tower-http = { version = "0.6", features = ["cors"], optional = true }
uuid = { version = "1", features = ["v4"], optional = true }
tokio-stream = { version = "0.1", optional = true }
wasm-bindgen = "0.2"
serde-wasm-bindgen = "0.6"
js-sys = "0.3"
getrandom = { version = "0.2", features = ["js"] }
console_error_panic_hook = "0.1"

[dev-dependencies]
tempfile = "3.8"

[features]
default = ["cli"]
cli = ["clap", "env_logger", "tabled", "colored", "tokenizers", "minijinja"]
serve = ["cli", "tokio", "axum", "tower-http", "uuid", "tokio-stream"]

[profile.release]
opt-level = "s" # Optimize for size in WASM
debug = true # Enable debug symbols for profiling
